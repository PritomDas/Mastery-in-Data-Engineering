Spark's Limitations
Spark has some limitation.


Spark Streamingâ€™s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record 
at a time. Native streaming tools such as Storm, Apex, or Flink can push down this latency value and might be more suitable for low-latency
applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's 
no need to add Spark to your stack of technologies.


Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale
linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with 
Tensorflow and other deep learning tools.


Hadoop versus Spark
The Hadoop ecosystem is a slightly older technology than the Spark ecosystem. In general, Hadoop MapReduce is slower than Spark because
Hadoop writes data out to disk during intermediate steps. However, many big companies, such as Facebook and LinkedIn, started using Big 
Data early and built their infrastructure around the Hadoop ecosystem.


While Spark is great for iterative algorithms, there is not much of a performance boost over Hadoop MapReduce when doing simple counting. Migrating legacy code to Spark, especially on hundreds of nodes that are already in production, might not be worth the cost for the small performance boost.


Beyond Spark for Storing and Processing Big Data
Keep in mind that Spark is not a data storage system, and there are a number of tools besides Spark that can be used to process and
analyze large datasets.


Sometimes it makes sense to use the power and simplicity of SQL on big data. For these cases, a new class of databases, know as NoSQL
and NewSQL, have been developed.


For example, you might hear about newer database storage systems like HBase or Cassandra. There are also distributed SQL engines like
Impala and Presto. Many of these technologies use query syntax that you are likely already familiar with based on your experiences with
Python and SQL.


In the lessons ahead, you will learn about Spark specifically, but know that many of the skills you already have with SQL, Python, 
and soon enough, Spark, will also be useful if you end up needing to learn any of these additional Big Data tools.
