As the name hints, accumulators are variables that accumulate. Because Spark runs in distributed mode, the workers are running 
in parallel, but asynchronously. For example, worker 1 will not be able to know how far worker 2 and worker 3 are done with their
tasks. With the same analogy, the variables that are local to workers are not going to be shared to another worker unless you accumulate
them. Accumulators are used for mostly sum operations, like in Hadoop MapReduce, but you can implement it to do otherwise.


More documentation: https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#accumulators
